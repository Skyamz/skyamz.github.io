---
date: 2020-09-06 12:00:00
layout: post
title: LSTM神经网络模型
subtitle: 长短期记忆网络
description: 
image: https://res.cloudinary.com/dm7h7e8xj/image/upload/v1559825288/theme17_nlndhx.jpg
optimized_image: https://res.cloudinary.com/dm7h7e8xj/image/upload/c_scale,w_380/v1559825288/theme17_nlndhx.jpg
category: work
tags:
  - work
  - career
author: mranderson
---

循环神经网络（RNN：Recurrent neural networks）是具有时间联结的前馈神经网络：它们有了状态，通道与通道之间有了时间上的联系。神经元的输入信息，不仅包括前一神经细胞层的输出，还包括它自身在先前通道的状态。

这就意味着：你的输入顺序将会影响神经网络的训练结果：相比先输入“曲奇饼”再输入“牛奶”，先输入“牛奶”再输入“曲奇饼”后，或许会产生不同的结果。RNN存在一大问题：梯度消失（或梯度爆炸，这取决于所用的激活函数），信息会随时间迅速消失，正如FFNN会随着深度的增加而失去信息一样。

直觉上，这不算什么大问题，因为这些都只是权重，而非神经元的状态，但随时间变化的权重正是来自过去信息的存储；如果权重是0或1000000，那之前的状态就不再有信息价值。

原则上，RNN可以在很多领域使用，因为大部分数据在形式上不存在时间线的变化，（不像语音或视频），它们能以某种序列的形式呈现出来。一张图片或一段文字可以一个像素或者一个文字地进行输入，因此，与时间相关的权重描述了该序列前一步发生了什么，而不是多少秒之前发生了什么。一般来说，循环神经网络是推测或补全信息很好的选择，比如自动补全。

## LSTM模型结构

长短期记忆（LSTM：Long / short term memory）网络试图通过引入门结构与明确定义的记忆单元来解决梯度消失/爆炸的问题。

这更多的是受电路图设计的启发，而非生物学上某种和记忆相关机制。每个神经元都有一个记忆单元和三个门：输入门、输出门、遗忘门。这三个门的功能就是通过禁止或允许信息流动来保护信息。

输入门决定了有多少前一神经细胞层的信息可留在当前记忆单元，输出层在另一端决定下一神经细胞层能从当前神经元获取多少信息。遗忘门乍看很奇怪，但有时候遗忘部分信息是很有用的：比如说它在学习一本书，并开始学一个新的章节，那遗忘前面章节的部分角色就很有必要了。

实践证明，LSTM可用来学习复杂的序列，比如像莎士比亚一样写作，或创作全新的音乐。值得注意的是，每一个门都对前一神经元的记忆单元赋有一个权重，因此会需要更多的计算资源。