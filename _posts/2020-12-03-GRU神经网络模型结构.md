---
date: 2020-12-03 12:00:00
layout: post
title: GRU模型结构
subtitle: 门循环神经网络模型.
description: 
image: https://res.cloudinary.com/dm7h7e8xj/image/upload/v1559824822/theme15_oqsl4z.jpg
optimized_image: https://res.cloudinary.com/dm7h7e8xj/image/upload/c_scale,w_380/v1559824822/theme15_oqsl4z.jpg
category: music
tags:
  - GRU
  - LSTM
  - RNN
author: mranderson
---
## LSTM
长短期记忆（LSTM：Long / short term memory）网络试图通过引入门结构与明确定义的记忆单元来解决梯度消失/爆炸的问题。

这更多的是受电路图设计的启发，而非生物学上某种和记忆相关机制。每个神经元都有一个记忆单元和三个门：输入门、输出门、遗忘门。这三个门的功能就是通过禁止或允许信息流动来保护信息。

输入门决定了有多少前一神经细胞层的信息可留在当前记忆单元，输出层在另一端决定下一神经细胞层能从当前神经元获取多少信息。遗忘门乍看很奇怪，但有时候遗忘部分信息是很有用的：比如说它在学习一本书，并开始学一个新的章节，那遗忘前面章节的部分角色就很有必要了。

实践证明，LSTM可用来学习复杂的序列，比如像莎士比亚一样写作，或创作全新的音乐。值得注意的是，每一个门都对前一神经元的记忆单元赋有一个权重，因此会需要更多的计算资源。

## GRU

门循环单元（GRU : Gated recurrent units）是LSTM的一种轻量级变体。它们少了一个门，同时连接方式也稍有不同：它们采用了一个更新门（update gate），而非LSTM所用的输入门、输出门、遗忘门。

更新门决定了保留多少上一个状态的信息，还决定了收取多少来自前一神经细胞层的信息。重置门（reset gate）跟LSTM遗忘门的功能很相似，但它存在的位置却稍有不同。它们总是输出完整的状态，没有输出门。多数情况下，它们跟LSTM类似，但最大的不同是：GRU速度更快、运行更容易（但函数表达力稍弱）。

在实践中，这里的优势和劣势会相互抵消：当你你需要更大的网络来获取函数表达力时，这样反过来，性能优势就被抵消了。在不需要额外的函数表达力时，GRU的综合性能要好于LSTM。